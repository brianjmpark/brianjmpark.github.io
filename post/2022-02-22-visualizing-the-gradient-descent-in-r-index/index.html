<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.72.0" />

  <title>Visualizing the gradient descent in R  &middot; Snow of London</title>

  <meta name="description" content="" />

  

<meta itemprop="name" content="Visualizing the gradient descent in R ">
<meta itemprop="description" content="The general idea of the gradient descent is to iteratively tweak parameters with the goal of minimizing a loss function.">
<meta itemprop="datePublished" content="2022-02-22T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2022-02-22T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1080">
<meta itemprop="image" content="https://brianjmpark.github.io/images/mont.jpg"/>



<meta itemprop="keywords" content="r,stats,ml," />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://brianjmpark.github.io/images/mont.jpg"/>

<meta name="twitter:title" content="Visualizing the gradient descent in R "/>
<meta name="twitter:description" content="The general idea of the gradient descent is to iteratively tweak parameters with the goal of minimizing a loss function."/>


<meta property="og:title" content="Visualizing the gradient descent in R " />
<meta property="og:description" content="The general idea of the gradient descent is to iteratively tweak parameters with the goal of minimizing a loss function." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://brianjmpark.github.io/post/2022-02-22-visualizing-the-gradient-descent-in-r-index/" />
<meta property="og:image" content="https://brianjmpark.github.io/images/mont.jpg"/>
<meta property="article:published_time" content="2022-02-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-02-22T00:00:00+00:00" /><meta property="og:site_name" content="Snow of London" />



  <link type="text/css"
        rel="stylesheet"
        href="/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="/css/hyde.css">

  
<style type="text/css">
  .sidebar {
    background-color: #b2c69c;
  }

  .read-more-link a {
    border-color: #b2c69c;
  }

  .pagination li a {
    color: #b2c69c;
    border: 1px solid #b2c69c;
  }

  .pagination li.active a {
    background-color: #b2c69c;
  }

  .pagination li a:hover {
    background-color: #b2c69c;
    opacity: 0.75;
  }

  footer a,
  .content a,
  .related-posts li a:hover {
    color: #b2c69c;
  }
</style>



  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="/images/mont.jpg" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Snow of London</h1>

      
      <p class="lead">Data, ML, and Sports Blog by Brian Jungmin Park</p>
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://brianjmpark.github.io">Home</a>
        </li>
        <li>
          <a href="/about/">About</a>
        </li><li>
          <a href="/contact/">Contact</a>
        </li><li>
          <a href="/posts/">Posts</a>
        </li><li>
          <a href="/categories/">Categories</a>
        </li><li>
          <a href="/tags/">Tags</a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://linkedin.com/in/bjp" rel="me" title="LinkedIn" target="_blank">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
      
      <a href="https://github.com/snowoflondon/" rel="me" title="GitHub" target="_blank">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://twitter.com/brianjmpark2" rel="me" title="Twitter" target="_blank">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    


  <main class="content container">
  <div class="post">
  <h1>Visualizing the gradient descent in R </h1>

  <div class="post-date">
    <time datetime="2022-02-22T00:00:00Z">Feb 22, 2022</time> &middot; 6 min read
  </div>

  


<p>The general idea of the gradient descent is to iteratively tweak parameters with the goal of minimizing a loss function. In the case of a linear regression model, the loss function is simply described by the mean-squared-error (MSE) - which is essentially the sum of squared residuals. The parameters to tweak in such a model would be the slope and the intercept of the regression line.</p>
<p>MSE is described by a quadratic function; this means that the further away you are from the minimum point, the steeper the slope. The concept of the gradient descent is to tweak the tangent line (which is described by the slope and the intercept) by some value (also known as the 'learning rate') until we've hit the inflection point of the parabola, that is, the minimum value of the loss function. A classic visualizer for the above problem is the ball rolling down the parabolic curve until it settles at the foot of the hill.</p>
<p>The gradient descent algorithm requires the function to be both differentiable and convex. The MSE function satisfies both criteria. The mathematical derivation of the gradient descent is described below:</p>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/eq1.png" />

</div>
<p>The response variable <span class="math inline">\(y\)</span> is a function of some predictor <span class="math inline">\(x_{i}\)</span>. <span class="math inline">\(\theta_i\)</span> describes the slope and <span class="math inline">\(\theta_0\)</span> describes the y-intercept. The loss function is the MSE.</p>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/eq2.png" />

</div>
<p>Firstly, the partial derivatives with respect to the slope and the intercept must be solved. The general solution for the partial derivative w.r.t both the slope and the intercept is shown above.</p>
<p>The result so far is the sum of residuals and a partial derivative term. The partial derivative term itself can be solved trivially for both the slope and the intercept as follows:</p>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/eq3.png" />

</div>
<p>Substituting the values for the respective partial derivatives, the equation we arrive at is:</p>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/eq4.png" />

</div>
<p>In other words, the derivative w.r.t the intercept term is simply the sum of residuals multiplied by a constant. The derivative w.r.t the slope term is the same, with an extra <span class="math inline">\(x_i\)</span> term. The partial derivatives then must be multiplied by the learning rate (i.e., the increment to which the parameters will be changed) to arrive at new values for the slope and the intercept. Once the calculated loss for the new set of parameters converge, the iterative process can terminate.</p>
<p>The gradient descent algorithm applied to a linear regression model can be demonstrated in R as follows:</p>
<pre class="r"><code>library(tidyverse)
set.seed(1000)

theta_0 &lt;- 5
theta_1 &lt;- 2
n_obs &lt;- 500
x &lt;- rnorm(n_obs)

y &lt;- theta_1*x + theta_0 + rnorm(n_obs, 0, 3)

rm(theta_0, theta_1)

data &lt;- tibble(x = x, y = y)

ggplot(data, aes(x = x, y = y)) + geom_point(size = 2) + theme_bw() + labs(title = &#39;Simulated Data&#39;)</code></pre>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/5.png" />

</div>
<pre class="r"><code>ols_fit &lt;- lm(y ~ x, data = data)
summary(ols_fit)
ols_fit$coefficients</code></pre>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/summary.png" />

</div>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/coef.png" />

</div>
<p>Above, we've created synthetic data and fit it to a linear model. The coefficients for the model (i.e., the slope and the intercept) are printed above. We can now write a function to calculate the loss:</p>
<pre class="r"><code>cost_function &lt;- function(theta_0, theta_1, x, y){
  pred &lt;- theta_1*x + theta_0
  res_sq &lt;- (y - pred)^2
  res_ss &lt;- sum(res_sq)
  return(mean(res_ss))
}

cost_function(theta_0 = ols_fit$coefficients[1][[1]], 
         theta_1 = ols_fit$coefficients[2][[1]],
         x = data$x, y = data$y)

sum(resid(ols_fit)^2)</code></pre>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/7.png" />

</div>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/7.png" />

</div>
<p>As above, the calculated loss corresponds to the MSE from the linear model.</p>
<p>The gradient descent function can be written as follows. Note that the <em>delta_theta_0</em> and the <em>delta_theta_1</em> correspond to the derived expressions for the partial derivatives earlier. We then write a subsequent function to apply the algorithm after defining our learning rate (alpha) and the number of times we will iterate through the algorithm.</p>
<pre class="r"><code>gradient_desc &lt;- function(theta_0, theta_1, x, y){
  N = length(x)
  pred &lt;- theta_1*x + theta_0
  res &lt;- y - pred
  delta_theta_0 &lt;- (2/N)*sum(res)
  delta_theta_1 &lt;- (2/N)*sum(res*x)
  return(c(delta_theta_0, delta_theta_1))
}

alpha &lt;- 0.1
iter &lt;- 100

minimize_function &lt;- function(theta_0, theta_1, x, y, alpha){
  gd &lt;- gradient_desc(theta_0, theta_1, x, y)
  d_theta_0 &lt;- gd[1] * alpha
  d_theta_1 &lt;- gd[2] * alpha
  new_theta_0 &lt;- theta_0 + d_theta_0
  new_theta_1 &lt;- theta_1 + d_theta_1
  return(c(new_theta_0, new_theta_1))
}</code></pre>
<p>The gradient descent algorithm needs a set of random initial values (i.e., initialization of the model). We will start at (0, 0) - that is, slope of 0 and intercept of 0 then progressively iterate through the gradient descent algorithm and see if we approach the coefficient values we got from the linear model earlier.</p>
<pre class="r"><code>res &lt;- list()
res[[1]] &lt;- c(0, 0)

for (i in 2:iter){
  res[[i]] &lt;- minimize_function(
    res[[i-1]][1], res[[i-1]][2], data$x, data$y, alpha
  )
}

res &lt;- lapply(res, function(x) as.data.frame(t(x))) %&gt;% bind_rows()
colnames(res) &lt;- c(&#39;theta0&#39;, &#39;theta1&#39;)

loss &lt;- res %&gt;% as_tibble() %&gt;% rowwise() %&gt;%
  summarise(mse = cost_function(theta0, theta1, data$x, data$y))

res &lt;- res %&gt;% bind_cols(loss) %&gt;%
  mutate(iteration = seq(1, 100)) %&gt;% as_tibble()

res %&gt;% glimpse()</code></pre>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/8.png" />

</div>
<p>From the data frame above, we can see that the values for the slope and the intercept are both approaching the values we got from the linear model (i.e., 4.898 and 2.085, respectively). The loss (MSE) also decreaes over iterations, as we expected. We can plot the loss versus the number of iterations to show this.</p>
<pre class="r"><code>ggplot(res, aes(x = iteration, y = mse)) + geom_point(size = 2) + 
  theme_classic() + geom_line(aes(group = 1)) </code></pre>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/9.png" />

</div>
<p>It looks like we didn't need anywhere near 100 iterations to converge at the minimum value.</p>
<p>Below, the iterative nature of the gradient descent is shown - the regression line at each iteration is plotted in red across our synthetic data. The blue line is the regression line at iteration = 1; a line with slope of zero and intercept of zero. Over time, the regression line is optimized until the green line is drawn - the regression line at iteration = 100, which describes our line of best fit.</p>
<pre class="r"><code>ggplot(data, aes(x = x, y = y)) + 
  geom_point(size = 2) + 
  geom_abline(aes(intercept = theta0, slope = theta1),
              data = res, size = 0.5, color = &#39;red&#39;) + 
  theme_classic() + 
  geom_abline(aes(intercept = theta0, slope = theta1), 
              data = res %&gt;% slice_head(), size = 0.5, color = &#39;blue&#39;) + 
  geom_abline(aes(intercept = theta0, slope = theta1), 
              data = res %&gt;% slice_tail(), size = 0.5, color = &#39;green&#39;)</code></pre>
<div class="figure">
<img src="/post/2022-02-22-visualizing-the-gradient-descent-in-r-index.en_files/10.png" />

</div>
<p>In summary, the gradient descent is an iterative algorithm which seeks the minimum value for a differentiable, convex function. This can be used to find the optimal values for a set of parameters by minimizing the model loss, as in the case of the linear regression model shown above. The formula for the gradient descent can be derived by using a set of partial derivatives with respect to each model parameter and applying a fixed learning rate.</p>

</div>


  </main>

  <footer>
  <div>
    &copy; Brian Jungmin Park 2023

    &middot; <a href="https://creativecommons.org/licenses/by-sa/4.0" target="_blank">CC BY-SA 4.0</a>

    &middot; Build with <a href="https://gohugo.io/" target="_blank">Hugo</a> & <a href="https://themes.gohugo.io/soho/" target="_blank">Soho</a> theme
  </div>
</footer>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
